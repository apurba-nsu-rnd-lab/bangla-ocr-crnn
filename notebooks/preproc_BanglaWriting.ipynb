{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-09 08:29:44--  https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/r43wkvdk4w-1.zip\n",
      "Resolving md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com (md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com)... 52.218.57.72\n",
      "Connecting to md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com (md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com)|52.218.57.72|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 268633599 (256M) [application/octet-stream]\n",
      "Saving to: ‘/home/ec2-user/word_level_ocr/pritom/datasets/handwriting/BanglaWriting.zip’\n",
      "\n",
      "100%[======================================>] 268,633,599 23.5MB/s   in 12s    \n",
      "\n",
      "2022-01-09 08:29:57 (21.4 MB/s) - ‘/home/ec2-user/word_level_ocr/pritom/datasets/handwriting/BanglaWriting.zip’ saved [268633599/268633599]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O \"/home/ec2-user/word_level_ocr/pritom/datasets/handwriting/BanglaWriting.zip\" \\\n",
    "      https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/r43wkvdk4w-1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "!unzip -qq \"/home/ec2-user/word_level_ocr/pritom/datasets/handwriting/BanglaWriting.zip\" -d \"/home/ec2-user/word_level_ocr/pritom/datasets/handwriting/\"\n",
    "!unzip -qq \"/home/ec2-user/word_level_ocr/pritom/datasets/handwriting/converted.zip\" -d \"/home/ec2-user/word_level_ocr/pritom/datasets/handwriting/BanglaWriting/\"\n",
    "!unzip -qq \"/home/ec2-user/word_level_ocr/pritom/datasets/handwriting/raw.zip\" -d \"/home/ec2-user/word_level_ocr/pritom/datasets/handwriting/BanglaWriting/\"\n",
    "\n",
    "os.remove(\"/home/ec2-user/word_level_ocr/pritom/datasets/handwriting/converted.zip\")\n",
    "os.remove(\"/home/ec2-user/word_level_ocr/pritom/datasets/handwriting/raw.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "DATA_DIR = \"/home/ec2-user/word_level_ocr/pritom/datasets/handwriting/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 520/520 [00:42<00:00, 12.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 520/520 [00:48<00:00, 10.83it/s]\n"
     ]
    }
   ],
   "source": [
    "WIDTH = 128\n",
    "HEIGHT = 512\n",
    "DIM = (WIDTH, HEIGHT)\n",
    "\n",
    "dataset_path = os.path.join(DATA_DIR, \"BanglaWriting\")\n",
    "extracted_path = os.path.join(DATA_DIR, \"BanglaWriting_words\")\n",
    "\n",
    "try:\n",
    "    # Create  Directory  MyDirectory \n",
    "    os.mkdir(extracted_path)\n",
    "except FileExistsError:\n",
    "    ##print if directory already exists\n",
    "    print(\"Directory already exists.\")\n",
    "\n",
    "for sub_dir in os.listdir(dataset_path):\n",
    "\n",
    "    dataset_sub_dir = os.path.join(dataset_path, sub_dir)\n",
    "\n",
    "    try:\n",
    "        # Create  Directory  MyDirectory \n",
    "        os.mkdir(os.path.join(extracted_path, sub_dir))\n",
    "    except FileExistsError:\n",
    "        ##print if directory already exists\n",
    "        print(\"Directory already exists.\")\n",
    "\n",
    "    word_counter = 0\n",
    "\n",
    "    for filename in tqdm(os.listdir(dataset_sub_dir)):\n",
    "        \n",
    "        if filename.endswith(\".json\"):\n",
    "\n",
    "            json_file = open(os.path.join(dataset_sub_dir, filename), encoding='utf-8')\n",
    "            page = json.load(json_file)\n",
    "\n",
    "            # take the image file name from the json file\n",
    "            # image_path = page[\"imagePath\"]\n",
    "\n",
    "            # if not image_path[0].isdigit():\n",
    "            #     image_path = image_path.rsplit(\"\\\\\", 1)[1]\n",
    "\n",
    "            image_path = filename.replace(\".json\", \".jpg\")\n",
    "            image = cv2.imread(os.path.join(dataset_sub_dir, image_path))\n",
    "            image_path = image_path.replace(\".jpg\", \"\")\n",
    "\n",
    "            # faulty_pages = [\"256_14_1/\", \"25_22_0/\"]\n",
    "            # if image_path in faulty_pages:okb ===============ntinue\n",
    "\n",
    "            # print(\"Extracting from \" + image_path + \" | words in page: \" + str(len(page[\"shapes\"])))\n",
    "\n",
    "            for i in range(len(page[\"shapes\"])):\n",
    "\n",
    "                word_counter += 1\n",
    "\n",
    "                label = page[\"shapes\"][i][\"label\"] # label of the word\n",
    "                # pixel coordinates of the bounding box\n",
    "                xmin, ymin = page[\"shapes\"][i][\"points\"][0]\n",
    "                xmax, ymax = page[\"shapes\"][i][\"points\"][1]\n",
    "\n",
    "                word = image[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
    "\n",
    "                # cv2.imshow(\"example\", word)\n",
    "                # cv2.waitKey(0)\n",
    "                # cv2.destroyAllWindows()\n",
    "                \n",
    "                if word.shape[0] == 0 or word.shape[1] == 0:\n",
    "                    continue\n",
    "                \n",
    "                word_resized = cv2.resize(word, DIM)\n",
    "                file_name = \"{}.jpg\".format(word_counter)\n",
    "                cv2.imwrite(os.path.join(extracted_path, sub_dir, file_name), word_resized)\n",
    "                \n",
    "                with open(os.path.join(extracted_path, \"{}_labels.txt\".format(sub_dir)), \"a\", encoding=\"utf8\") as f:\n",
    "                    f.write(\"{} {}\\n\".format(file_name, label))\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Validation Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_path = os.path.join(DATA_DIR, \"BanglaWriting_words\")\n",
    "\n",
    "labels_files = glob.glob(extracted_path + \"/*labels.txt\")\n",
    "\n",
    "for label_file in labels_files:\n",
    "    current_dir = label_file.rsplit('/', 1)[1].split('_', 1)[0]\n",
    "\n",
    "    with open(label_file) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    random.shuffle(lines)\n",
    "\n",
    "    train_split = int(0.90 * len(lines))\n",
    "    valid_split = train_split + int(0.05 * len(lines))\n",
    "\n",
    "    train_data = lines[:train_split]\n",
    "    valid_data = lines[train_split:valid_split]\n",
    "    test_data = lines[valid_split:]\n",
    "\n",
    "    with open(os.path.join(extracted_path, \"{}_labels_train.txt\".format(current_dir)), 'w') as fout:\n",
    "        for line in train_data:\n",
    "            fout.write(line)\n",
    "\n",
    "    with open(os.path.join(extracted_path, \"{}_labels_valid.txt\".format(current_dir)), 'w') as fout:\n",
    "        for line in valid_data:\n",
    "            fout.write(line)\n",
    "\n",
    "    with open(os.path.join(extracted_path, \"{}_labels_test.txt\".format(current_dir)), 'w') as fout:\n",
    "        for line in test_data:\n",
    "            fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p39)",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
